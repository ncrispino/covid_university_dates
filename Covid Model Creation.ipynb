{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Predict how long it takes for colleges to implement vaccine requirement. This notebook also includes preprocessing to be used in all future analyses with these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Engineering\n",
    "Some was done in previous notebook, but most will be done here. I'm using the cleaned vaccine data from my 'Vaccine Mandates' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>College</th>\n",
       "      <th>ranking</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>zip</th>\n",
       "      <th>announce_date</th>\n",
       "      <th>Type</th>\n",
       "      <th>State_x</th>\n",
       "      <th>all_employee_vacc</th>\n",
       "      <th>some_employee_vacc</th>\n",
       "      <th>...</th>\n",
       "      <th>Region</th>\n",
       "      <th>Division</th>\n",
       "      <th>zip_str</th>\n",
       "      <th>cleaned_name_list</th>\n",
       "      <th>2020.student.size</th>\n",
       "      <th>school.name</th>\n",
       "      <th>school.zip</th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_school.name_list</th>\n",
       "      <th>name_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelphi University</td>\n",
       "      <td>171</td>\n",
       "      <td>NY</td>\n",
       "      <td>Garden City</td>\n",
       "      <td>11530</td>\n",
       "      <td>116.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>NY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>11530</td>\n",
       "      <td>['adelphi']</td>\n",
       "      <td>5076.0</td>\n",
       "      <td>Adelphi University</td>\n",
       "      <td>11530</td>\n",
       "      <td>188429.0</td>\n",
       "      <td>['adelphi']</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American University</td>\n",
       "      <td>78</td>\n",
       "      <td>DC</td>\n",
       "      <td>Washington</td>\n",
       "      <td>20016</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>DC</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>South</td>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>20016</td>\n",
       "      <td>['american']</td>\n",
       "      <td>7510.0</td>\n",
       "      <td>American University</td>\n",
       "      <td>20016</td>\n",
       "      <td>131159.0</td>\n",
       "      <td>['american']</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona State University</td>\n",
       "      <td>116</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Tempe</td>\n",
       "      <td>85287</td>\n",
       "      <td>197.0</td>\n",
       "      <td>Public</td>\n",
       "      <td>AZ</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>West</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>85287</td>\n",
       "      <td>['arizona', 'state']</td>\n",
       "      <td>62633.0</td>\n",
       "      <td>Arizona State University Campus Immersion</td>\n",
       "      <td>85287</td>\n",
       "      <td>104151.0</td>\n",
       "      <td>['arizona', 'state', 'immersion']</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aurora University</td>\n",
       "      <td>302</td>\n",
       "      <td>IL</td>\n",
       "      <td>Aurora</td>\n",
       "      <td>60506</td>\n",
       "      <td>139.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>IL</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>East North Central</td>\n",
       "      <td>60506</td>\n",
       "      <td>['aurora']</td>\n",
       "      <td>4123.0</td>\n",
       "      <td>Aurora University</td>\n",
       "      <td>60506</td>\n",
       "      <td>143118.0</td>\n",
       "      <td>['aurora']</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bellarmine University</td>\n",
       "      <td>202</td>\n",
       "      <td>KY</td>\n",
       "      <td>Louisville</td>\n",
       "      <td>40205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Private</td>\n",
       "      <td>KY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>South</td>\n",
       "      <td>East South Central</td>\n",
       "      <td>40205</td>\n",
       "      <td>['bellarmine']</td>\n",
       "      <td>2431.0</td>\n",
       "      <td>Bellarmine University</td>\n",
       "      <td>40205</td>\n",
       "      <td>156286.0</td>\n",
       "      <td>['bellarmine']</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    College  ranking state         city    zip  announce_date  \\\n",
       "0        Adelphi University      171    NY  Garden City  11530          116.0   \n",
       "1       American University       78    DC   Washington  20016           13.0   \n",
       "2  Arizona State University      116    AZ        Tempe  85287          197.0   \n",
       "3         Aurora University      302    IL       Aurora  60506          139.0   \n",
       "4     Bellarmine University      202    KY   Louisville  40205            NaN   \n",
       "\n",
       "      Type State_x  all_employee_vacc  some_employee_vacc  ...     Region  \\\n",
       "0  Private      NY                  0                   0  ...  Northeast   \n",
       "1  Private      DC                  1                   0  ...      South   \n",
       "2   Public      AZ                  1                   0  ...       West   \n",
       "3  Private      IL                  1                   0  ...    Midwest   \n",
       "4  Private      KY                  1                   0  ...      South   \n",
       "\n",
       "             Division  zip_str     cleaned_name_list  2020.student.size  \\\n",
       "0     Middle Atlantic    11530           ['adelphi']             5076.0   \n",
       "1      South Atlantic    20016          ['american']             7510.0   \n",
       "2            Mountain    85287  ['arizona', 'state']            62633.0   \n",
       "3  East North Central    60506            ['aurora']             4123.0   \n",
       "4  East South Central    40205        ['bellarmine']             2431.0   \n",
       "\n",
       "                                 school.name  school.zip        id  \\\n",
       "0                         Adelphi University       11530  188429.0   \n",
       "1                        American University       20016  131159.0   \n",
       "2  Arizona State University Campus Immersion       85287  104151.0   \n",
       "3                          Aurora University       60506  143118.0   \n",
       "4                      Bellarmine University       40205  156286.0   \n",
       "\n",
       "            cleaned_school.name_list  name_similarity  \n",
       "0                        ['adelphi']         1.000000  \n",
       "1                       ['american']         1.000000  \n",
       "2  ['arizona', 'state', 'immersion']         0.816497  \n",
       "3                         ['aurora']         1.000000  \n",
       "4                     ['bellarmine']         1.000000  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacc_data = pd.read_csv('vacc_mandates_cleaned_school.csv')\n",
    "vacc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, drop unnecessary columns (i.e., ones that won't be used as features in my model). Drop names, locations (as I extracted data from them) and ```state_pol```, as that was from the Chronicle data but I found different political data myself. Also dropping records of employee vaccination and boosters, as I'm trying to predict original student vaccination. I will consider ```all_students_vacc``` instead of ```res_students_vacc``` as it'll be less dependent on each college's dorm situations. Also drop ```Division``` as I don't have that much data, so using the more broad category of ```Region``` will probably be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacc_data.drop(columns=['College', 'state', 'city', 'zip', 'State_x', 'state_fips', \n",
    "                        'STCOUNTYFP', 'county_fips', 'county_fips_str', 'state_pol', \n",
    "                        'State_y', 'State Code', 'Division', 'zip_str', 'cleaned_name_list',\n",
    "                        'school.name', 'school.zip', 'id', 'cleaned_school.name_list', 'name_similarity',\n",
    "                        'all_employee_vacc', 'some_employee_vacc', 'res_students_vacc'], \n",
    "               inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ranking</th>\n",
       "      <th>announce_date</th>\n",
       "      <th>Type</th>\n",
       "      <th>all_students_vacc</th>\n",
       "      <th>booster</th>\n",
       "      <th>median_income</th>\n",
       "      <th>total_population</th>\n",
       "      <th>avg_hhsize</th>\n",
       "      <th>avg_community_level</th>\n",
       "      <th>political_control_state</th>\n",
       "      <th>county_vote_diff</th>\n",
       "      <th>Region</th>\n",
       "      <th>2020.student.size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171</td>\n",
       "      <td>116.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47254.0</td>\n",
       "      <td>1355683.0</td>\n",
       "      <td>2.62</td>\n",
       "      <td>0.624352</td>\n",
       "      <td>Dem</td>\n",
       "      <td>0.059304</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>5076.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52328.0</td>\n",
       "      <td>701974.0</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>Dem</td>\n",
       "      <td>0.867763</td>\n",
       "      <td>South</td>\n",
       "      <td>7510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>197.0</td>\n",
       "      <td>Public</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34032.0</td>\n",
       "      <td>4412779.0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.492925</td>\n",
       "      <td>Rep</td>\n",
       "      <td>-0.028354</td>\n",
       "      <td>West</td>\n",
       "      <td>62633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>302</td>\n",
       "      <td>139.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35433.0</td>\n",
       "      <td>531756.0</td>\n",
       "      <td>2.81</td>\n",
       "      <td>0.568831</td>\n",
       "      <td>Dem</td>\n",
       "      <td>0.105015</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>4123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Private</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32123.0</td>\n",
       "      <td>768419.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.062827</td>\n",
       "      <td>Div</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>South</td>\n",
       "      <td>2431.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ranking  announce_date     Type  all_students_vacc  booster  median_income  \\\n",
       "0      171          116.0  Private                  1        0        47254.0   \n",
       "1       78           13.0  Private                  1        1        52328.0   \n",
       "2      116          197.0   Public                  0        0        34032.0   \n",
       "3      302          139.0  Private                  1        0        35433.0   \n",
       "4      202            NaN  Private                  1        0        32123.0   \n",
       "\n",
       "   total_population  avg_hhsize  avg_community_level political_control_state  \\\n",
       "0         1355683.0        2.62             0.624352                     Dem   \n",
       "1          701974.0        2.19             0.726562                     Dem   \n",
       "2         4412779.0        2.64             0.492925                     Rep   \n",
       "3          531756.0        2.81             0.568831                     Dem   \n",
       "4          768419.0        2.25             1.062827                     Div   \n",
       "\n",
       "   county_vote_diff     Region  2020.student.size  \n",
       "0          0.059304  Northeast             5076.0  \n",
       "1          0.867763      South             7510.0  \n",
       "2         -0.028354       West            62633.0  \n",
       "3          0.105015    Midwest             4123.0  \n",
       "4          0.133300      South             2431.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As rankings are tied from 299-391 (see \"Vaccine Mandates.ipynb\") and a change in ranking won't usually signal a large change in a college's operations unless they move to a new echelon, I will place the rankings into bins. The rankings are fairly arbitrary besides 299-391. I isolated the top 20 because they often are spoken of in a different way. Then, I separated the rest of the top 100 from above 100 (another arbitrary cut, but one that makes sense when talking about top colleges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacc_data['ranking'] = pd.cut(vacc_data['ranking'], bins=[0, 20, 100, 200, 298, 400], labels=['a', 'b', 'c', 'd', 'e'], right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I need to figure out my target variable. Explore the two possibilities--categorical variable for vaccination status or continuous variable for days after first announcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    141\n",
       "0     12\n",
       "Name: all_students_vacc, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacc_data['all_students_vacc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_students_vacc  booster\n",
       "1                  0          99\n",
       "                   1          42\n",
       "0                  0          12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacc_data[['all_students_vacc', 'booster']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacc_data['announce_date'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacc_data.dropna(subset=['announce_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 14 colleges not requiring the vaccine, there's likely not enough data to train a good model. So, I can drop these and do a regression estimating how long it takes for a college to institute a vaccination requirement, under the assumption that they will make one. This won't be as useful, but can help gauge each college's decision times and urgency regarding their requirements.\n",
    "\n",
    "Note that in the future, I can also try to predict if a college that already had a vaccine requirement will require a booster. Svae this for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date = vacc_data['announce_date'] # Note: is continuous\n",
    "features_date = vacc_data.drop(columns=['announce_date', 'all_students_vacc', 'booster'])\n",
    "target_booster = vacc_data['booster'] # Note: is binary\n",
    "features_booster = vacc_data.drop(columns=['all_students_vacc', 'booster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_booster.to_pickle('target_booster.pkl')\n",
    "features_booster.to_pickle('features_booster.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make sure all the vars are the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ranking                    category\n",
       "announce_date               float64\n",
       "Type                         object\n",
       "all_students_vacc             int64\n",
       "booster                       int64\n",
       "median_income               float64\n",
       "total_population            float64\n",
       "avg_hhsize                  float64\n",
       "avg_community_level         float64\n",
       "political_control_state      object\n",
       "county_vote_diff            float64\n",
       "Region                       object\n",
       "2020.student.size           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacc_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ranking', 'announce_date', 'Type', 'all_students_vacc', 'booster',\n",
       "       'median_income', 'total_population', 'avg_hhsize',\n",
       "       'avg_community_level', 'political_control_state', 'county_vote_diff',\n",
       "       'Region', '2020.student.size'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vacc_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use one-hot encoding for the categorical variables. Note that my preprocessing is copied/based on [this sklearn course](https://inria.github.io/scikit-learn-mooc/python_scripts/03_categorical_pipeline_column_transformer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "categorical_preprocessor = OneHotEncoder(drop='first') # drop to avoid multicollinearity\n",
    "numerical_preprocessor = StandardScaler() # normalize data to make it easier for sklearn models to handle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "type_encoded = categorical_preprocessor.fit_transform(vacc_data[['Type']])\n",
    "print(type_encoded.toarray()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a pipeline, apply to all categorical features. Also, standardize numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['ranking', 'Type', 'political_control_state', 'Region']\n",
    "numerical_columns = list(set(features_date.columns).difference(categorical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer # splits the column, transforms each subset differently, then concatenates\n",
    "preprocessor = ColumnTransformer([('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "                                  ('standard_scaler', numerical_preprocessor, numerical_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into testing and training (which randomizes data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_date, target_date, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Use mean squared error, as dates that are much further away are significantly worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use normal linear regression first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                  OneHotEncoder(drop='first'),\n",
       "                                                  ['ranking', 'Type',\n",
       "                                                   'political_control_state',\n",
       "                                                   'Region']),\n",
       "                                                 ('standard_scaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['total_population',\n",
       "                                                   'avg_community_level',\n",
       "                                                   'county_vote_diff',\n",
       "                                                   'median_income',\n",
       "                                                   '2020.student.size',\n",
       "                                                   'avg_hhsize'])])),\n",
       "                ('linearregression', LinearRegression())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe_linear = make_pipeline(preprocessor, LinearRegression())\n",
    "pipe_linear.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4361647632216752"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_linear.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1981.728506693466"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_linear = pipe_linear.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare against dummy estimator for sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: -0.00040652144083153097, 3516.1586089391376\n",
      "median: -0.14004089843234668, 4006.9357142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "for strategy in ['mean', 'median']:\n",
    "    dummy_regr = DummyRegressor(strategy=strategy)\n",
    "    dummy_regr.fit(X_train, y_train)\n",
    "    y_pred_dummy = dummy_regr.predict(X_test)\n",
    "    print(f'{strategy}: {dummy_regr.score(X_test, y_test)}, {mean_squared_error(y_test, y_pred_dummy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My predictions are much better, meaning they have some actual degree of explanatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though there are no hyperparameters to tune, an example of cross validation is shown as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05902675,  0.41527914,  0.24022537,  0.46778222,  0.36731294])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(pipe_linear, X_train, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add regularization--let's try Lasso as some features may note be important, using cross validation to select the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                  OneHotEncoder(drop='first'),\n",
       "                                                  ['ranking', 'Type',\n",
       "                                                   'political_control_state',\n",
       "                                                   'Region']),\n",
       "                                                 ('standard_scaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['total_population',\n",
       "                                                   'avg_community_level',\n",
       "                                                   'county_vote_diff',\n",
       "                                                   'median_income',\n",
       "                                                   '2020.student.size',\n",
       "                                                   'avg_hhsize'])])),\n",
       "                ('lassocv', LassoCV())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "pipe_lasso = make_pipeline(preprocessor, LassoCV())\n",
    "pipe_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40397416700424804"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lasso.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2094.8697543672342"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lasso = pipe_lasso.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also not working too well. Try ensemble learning--specifically, a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                  OneHotEncoder(drop='first'),\n",
       "                                                  ['ranking', 'Type',\n",
       "                                                   'political_control_state',\n",
       "                                                   'Region']),\n",
       "                                                 ('standard_scaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['total_population',\n",
       "                                                   'avg_community_level',\n",
       "                                                   'county_vote_diff',\n",
       "                                                   'median_income',\n",
       "                                                   '2020.student.size',\n",
       "                                                   'avg_hhsize'])])),\n",
       "                ('randomforestregressor',\n",
       "                 RandomForestRegressor(random_state=42))])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "pipe_forest = make_pipeline(preprocessor, RandomForestRegressor(random_state=42))\n",
    "pipe_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(model.score(X_test, y_test))\n",
    "    print(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1839001172674115\n",
      "2868.3705742857146\n"
     ]
    }
   ],
   "source": [
    "show_metrics(pipe_forest, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the coefficient of determination and the mean squared error are relatively high.\n",
    "\n",
    "So, I'll adjust the hyperparameters by using CV (as my dataset is small so fitting won't be too costly). Note that I could use OOB as well, which I'll do first as the model already calculates it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'columntransformer', 'randomforestregressor', 'columntransformer__n_jobs', 'columntransformer__remainder', 'columntransformer__sparse_threshold', 'columntransformer__transformer_weights', 'columntransformer__transformers', 'columntransformer__verbose', 'columntransformer__verbose_feature_names_out', 'columntransformer__one-hot-encoder', 'columntransformer__standard_scaler', 'columntransformer__one-hot-encoder__categories', 'columntransformer__one-hot-encoder__drop', 'columntransformer__one-hot-encoder__dtype', 'columntransformer__one-hot-encoder__handle_unknown', 'columntransformer__one-hot-encoder__sparse', 'columntransformer__standard_scaler__copy', 'columntransformer__standard_scaler__with_mean', 'columntransformer__standard_scaler__with_std', 'randomforestregressor__bootstrap', 'randomforestregressor__ccp_alpha', 'randomforestregressor__criterion', 'randomforestregressor__max_depth', 'randomforestregressor__max_features', 'randomforestregressor__max_leaf_nodes', 'randomforestregressor__max_samples', 'randomforestregressor__min_impurity_decrease', 'randomforestregressor__min_samples_leaf', 'randomforestregressor__min_samples_split', 'randomforestregressor__min_weight_fraction_leaf', 'randomforestregressor__n_estimators', 'randomforestregressor__n_jobs', 'randomforestregressor__oob_score', 'randomforestregressor__random_state', 'randomforestregressor__verbose', 'randomforestregressor__warm_start'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_forest.get_params().keys() # possible grid search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'randomforestregressor__n_estimators': [25, 75, 100, 150, 200],\n",
    "     'randomforestregressor__max_depth': [None, 5, 10, 15],\n",
    "     'randomforestregressor__max_features': [None, 0.25, 0.5, 0.75], # Note I have 10 features (when they're not encoded)\n",
    "     'randomforestregressor__max_leaf_nodes': [None, 2, 5, 10], \n",
    "     'randomforestregressor__max_samples': [None, 25, 50, 75] # 104 samples in training data -- 84 in validation\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm copying code from [Ben Reiniger's answer from StackExchange](https://datascience.stackexchange.com/questions/66216/gridsearch-without-cv/66238#66238) below. Need to create a metric that measures OOB then pass it to ```GridSearchCV```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "cv = PredefinedSplit([-1]*(X_train.shape[0]-1) + [0])\n",
    "for (train, test) in cv.split(X_train, y_train):\n",
    "    print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1,  0])),\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                                         OneHotEncoder(drop='first'),\n",
       "                                                                         ['ranking',\n",
       "                                                                          'Type',\n",
       "                                                                          'political_control_state',\n",
       "                                                                          'Region']),\n",
       "                                                                        ('standard_scaler',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['total_population',\n",
       "                                                                          'avg_community_level',\n",
       "                                                                          'county_vote_diff',\n",
       "                                                                          'medi...\n",
       "                                                              random_state=42))]),\n",
       "             param_grid=[{'randomforestregressor__max_depth': [None, 5, 10, 15],\n",
       "                          'randomforestregressor__max_features': [None, 0.25,\n",
       "                                                                  0.5, 0.75],\n",
       "                          'randomforestregressor__max_leaf_nodes': [None, 2, 5,\n",
       "                                                                    10],\n",
       "                          'randomforestregressor__max_samples': [None, 25, 50,\n",
       "                                                                 75],\n",
       "                          'randomforestregressor__n_estimators': [25, 75, 100,\n",
       "                                                                  150, 200]}],\n",
       "             scoring=<function oob_scorer at 0x000001D74A3EC4C8>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def oob_scorer(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Get oob score where RandomForest is 1st element in Pipeline\n",
    "    \"\"\"\n",
    "    return estimator[1].oob_score_\n",
    "pipe_forest_oob = GridSearchCV(estimator=make_pipeline(preprocessor, RandomForestRegressor(random_state=42, oob_score=True)), \n",
    "                               param_grid=param_grid, scoring=oob_scorer, cv=cv)\n",
    "pipe_forest_oob.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestregressor__max_depth': 5,\n",
       " 'randomforestregressor__max_features': 0.75,\n",
       " 'randomforestregressor__max_leaf_nodes': 10,\n",
       " 'randomforestregressor__max_samples': None,\n",
       " 'randomforestregressor__n_estimators': 150}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_forest_oob.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2651624407049967\n",
      "2866.8761660137266\n"
     ]
    }
   ],
   "source": [
    "show_metrics(pipe_forest_oob, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there's a small improvement using grid search and oob error. Note that takes a short amount of time as no validation set is needed--the parts of the training data that are not used in creating each weak learner are used to get a fairly unbiased estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do CV, which should be similar but take longer, as training and evaluation is needed for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                                         OneHotEncoder(drop='first'),\n",
       "                                                                         ['ranking',\n",
       "                                                                          'Type',\n",
       "                                                                          'political_control_state',\n",
       "                                                                          'Region']),\n",
       "                                                                        ('standard_scaler',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['total_population',\n",
       "                                                                          'avg_community_level',\n",
       "                                                                          'county_vote_diff',\n",
       "                                                                          'median_income',\n",
       "                                                                          '2020.student.size',\n",
       "                                                                          'avg_hhsize'])])),\n",
       "                                       ('randomforestregressor',\n",
       "                                        RandomForestRegressor(random_state=42))]),\n",
       "             param_grid=[{'randomforestregressor__max_depth': [None, 5, 10, 15],\n",
       "                          'randomforestregressor__max_features': [None, 0.25,\n",
       "                                                                  0.5, 0.75],\n",
       "                          'randomforestregressor__max_leaf_nodes': [None, 2, 5,\n",
       "                                                                    10],\n",
       "                          'randomforestregressor__max_samples': [None, 25, 50,\n",
       "                                                                 75],\n",
       "                          'randomforestregressor__n_estimators': [25, 75, 100,\n",
       "                                                                  150, 200]}])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_forest_grid = GridSearchCV(make_pipeline(preprocessor, RandomForestRegressor(random_state=42)), param_grid)\n",
    "pipe_forest_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestregressor__max_depth': None,\n",
       " 'randomforestregressor__max_features': 0.75,\n",
       " 'randomforestregressor__max_leaf_nodes': 10,\n",
       " 'randomforestregressor__max_samples': 75,\n",
       " 'randomforestregressor__n_estimators': 75}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_forest_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1699791929868104"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_forest_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2917.298861641467"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_forest_grid = pipe_forest_grid.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred_forest_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular grid search takes a long time to run. My dataset is fairly small, so I'm ok with this. However, if it was larger, I would use either ```RandomizedSearchCV```,  ```HalvingGridSearchCV``` or ```HalvingRandomSearchCV```. \n",
    "\n",
    "Here I'll use ```HalvingGridSearchCV```. This method starts by evaluating each model with a small amount of resources (e.g., a small number of ```n_estimators``` or samples) and chooses the best models from there. It increases the resources by multiplying it by ```factor``` and decreases the number of candidate models by dividing it by ```factor```. We will set ```min_resources='exhaust'``` so that the last iteration uses all resources possible, set by ```max_resources```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_forest_no_oob = make_pipeline(preprocessor, RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HalvingGridSearchCV(estimator=Pipeline(steps=[('columntransformer',\n",
       "                                               ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                                                OneHotEncoder(drop='first'),\n",
       "                                                                                ['ranking',\n",
       "                                                                                 'Type',\n",
       "                                                                                 'political_control_state',\n",
       "                                                                                 'Region']),\n",
       "                                                                               ('standard_scaler',\n",
       "                                                                                StandardScaler(),\n",
       "                                                                                ['total_population',\n",
       "                                                                                 'avg_community_level',\n",
       "                                                                                 'county_vote_diff',\n",
       "                                                                                 'median_income',\n",
       "                                                                                 '2020.student.size',\n",
       "                                                                                 'avg_hhsize'])])),\n",
       "                                              ('randomforestregressor',\n",
       "                                               RandomForestRegressor())]),\n",
       "                    factor=2, max_resources=200,\n",
       "                    param_grid=[{'randomforestregressor__max_depth': [None, 5,\n",
       "                                                                      10, 15],\n",
       "                                 'randomforestregressor__max_features': [None,\n",
       "                                                                         0.25,\n",
       "                                                                         0.5,\n",
       "                                                                         0.75],\n",
       "                                 'randomforestregressor__max_leaf_nodes': [None,\n",
       "                                                                           2, 5,\n",
       "                                                                           10],\n",
       "                                 'randomforestregressor__max_samples': [None,\n",
       "                                                                        25, 50,\n",
       "                                                                        75]}],\n",
       "                    resource='randomforestregressor__n_estimators')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "param_grid_halving = [\n",
    "    {'randomforestregressor__max_depth': [None, 5, 10, 15],\n",
    "     'randomforestregressor__max_features': [None, 0.25, 0.5, 0.75], # Note I have 10 features (when they're not encoded)\n",
    "     'randomforestregressor__max_leaf_nodes': [None, 2, 5, 10], \n",
    "     'randomforestregressor__max_samples': [None, 25, 50, 75] # 104 samples in training data -- 84 in validation\n",
    "    }\n",
    "]\n",
    "pipe_forest_halving_grid = HalvingGridSearchCV(pipe_forest_no_oob, param_grid_halving, cv=5, factor=2,\n",
    "                                               resource='randomforestregressor__n_estimators', max_resources=200)\n",
    "pipe_forest_halving_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestregressor__max_depth': 5,\n",
       " 'randomforestregressor__max_features': 0.75,\n",
       " 'randomforestregressor__max_leaf_nodes': 10,\n",
       " 'randomforestregressor__max_samples': None,\n",
       " 'randomforestregressor__n_estimators': 128}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_forest_halving_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1757530795905723"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_forest_halving_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2897.0052103569706"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_forest_halving_grid = pipe_forest_halving_grid.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred_forest_halving_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using halving grid search with my specified parameters produces a model that's about equivalent to my original random forest. Change the parameters a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HalvingGridSearchCV(estimator=Pipeline(steps=[('columntransformer',\n",
       "                                               ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                                                OneHotEncoder(drop='first'),\n",
       "                                                                                ['ranking',\n",
       "                                                                                 'Type',\n",
       "                                                                                 'political_control_state',\n",
       "                                                                                 'Region']),\n",
       "                                                                               ('standard_scaler',\n",
       "                                                                                StandardScaler(),\n",
       "                                                                                ['total_population',\n",
       "                                                                                 'avg_community_level',\n",
       "                                                                                 'county_vote_diff',\n",
       "                                                                                 'median_income',\n",
       "                                                                                 '2020.student.size',\n",
       "                                                                                 'avg_hhsize'])])),\n",
       "                                              ('randomforestregressor',\n",
       "                                               RandomForestRegressor())]),\n",
       "                    factor=2, max_resources=256,\n",
       "                    param_grid=[{'randomforestregressor__max_depth': [8, 9, 10,\n",
       "                                                                      11, 12],\n",
       "                                 'randomforestregressor__max_features': [0.5,\n",
       "                                                                         0.6,\n",
       "                                                                         0.75,\n",
       "                                                                         0.8,\n",
       "                                                                         0.9,\n",
       "                                                                         1.0],\n",
       "                                 'randomforestregressor__max_leaf_nodes': [4, 5,\n",
       "                                                                           7,\n",
       "                                                                           8],\n",
       "                                 'randomforestregressor__max_samples': [None,\n",
       "                                                                        70, 75,\n",
       "                                                                        80]}],\n",
       "                    resource='randomforestregressor__n_estimators')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_halving_new = [\n",
    "    {'randomforestregressor__max_depth': [8, 9, 10, 11, 12],\n",
    "     'randomforestregressor__max_features': [0.5, 0.6, 0.75, 0.8, 0.9, 1.0], # Note I have 10 features (when they're not encoded)\n",
    "     'randomforestregressor__max_leaf_nodes': [4, 5, 7, 8], \n",
    "     'randomforestregressor__max_samples': [None, 70, 75, 80] # 104 samples in training data -- 84 in validation\n",
    "    }\n",
    "]\n",
    "pipe_forest_halving_grid_new = HalvingGridSearchCV(pipe_forest_no_oob, param_grid_halving_new, cv=5, factor=2,\n",
    "                                               resource='randomforestregressor__n_estimators', max_resources=256)\n",
    "pipe_forest_halving_grid_new.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestregressor__max_depth': 5,\n",
       " 'randomforestregressor__max_features': 0.75,\n",
       " 'randomforestregressor__max_leaf_nodes': 10,\n",
       " 'randomforestregressor__max_samples': None,\n",
       " 'randomforestregressor__n_estimators': 128}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_forest_halving_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1757530795905723"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_forest_halving_grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2897.0052103569706"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_forest_halving_grid = pipe_forest_halving_grid.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred_forest_halving_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using halving grid actually turned out worse on my test set than my base random forest model, but running my original grid search turned out best and ended up not taking too long (not over 15min).\n",
    "\n",
    "Based on these results, I don't think using a random forest is best. Maybe I could improve it more, but based on my metrics, I am far off from the results with linear regression. So, original linear regression seems the best, likely due to our small sample and its good generalization. \n",
    "\n",
    "I'll try one more regression model before moving on -- SVM regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: My test set is not a true test set if we define it to be an unbiased estimate of the model's performance on new data. This is because I've used it to choose the best model--each model can be considered a hyperparameter that I'm tuning, so the test set is no longer a truly unbiased estimate.\n",
    "\n",
    "See these answers for more:\n",
    "- https://stats.stackexchange.com/questions/540454/is-it-wrong-to-compare-multiple-models-on-the-same-test-set-and-choose-the-best\n",
    "- https://datascience.stackexchange.com/questions/43210/can-i-use-the-test-dataset-to-select-a-model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maybe should output a rank (i.e., which universities acted first)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "- https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "- https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3\n",
    "- https://scikit-learn.org/stable/model_selection.html\n",
    "- https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "deep_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
