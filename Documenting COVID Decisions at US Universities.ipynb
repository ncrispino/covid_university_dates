{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "Create a dataset with the dates of the top 100 universities' covid decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolating the Top Universities (with individual characteristics)\n",
    "I'll first isolate the top 100 universities from US NEWS. The below is all copied from [nahid18's uni-rank python package](https://github.com/nahid18/uni-rank). However, because I got a list index out of range error when using the package, I copied their code and fixed it locally. The problem was in the get_usa method, solved by changing the '/' to os.sep. I then saved the results in a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pkgutil\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "class Ranking:\n",
    "    def __init__(self):\n",
    "        self.api = \"https://www.usnews.com/best-colleges/api/search?_sort=rank&_sortDirection=asc&schoolType=national-universities\"\n",
    "\n",
    "\n",
    "    def _useragent(self):\n",
    "        agents = pkgutil.get_data(__package__, \"useragents.txt\").decode(\"utf-8\")\n",
    "        return random.choice(agents.splitlines())\n",
    "\n",
    "\n",
    "    def __save_initial(self):\n",
    "        usnews = header_selected = dict()\n",
    "        header_selected = dict()\n",
    "        fetched = False\n",
    "        while fetched != True:\n",
    "            try:\n",
    "                ua = self._useragent()\n",
    "                headers = {\"User-Agent\": ua}\n",
    "                data = requests.get(url=self.api, headers=headers, timeout=3)\n",
    "                if data.status_code == 200:\n",
    "                    header_selected = headers\n",
    "                    usnews = data.json()\n",
    "                    with open(\"apidata/1.json\", 'w') as fw:\n",
    "                        json.dump(usnews, fw)\n",
    "                    fetched = True\n",
    "                    return {\"totalPages\": usnews['data']['totalPages'], \"head\": header_selected}\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    def __save_rest(self, length, head):\n",
    "        for page in range(2, length+1):\n",
    "            url = self.api+\"&_page=\"+str(page)\n",
    "            try:\n",
    "                page_raw = requests.get(url=url, headers=head, timeout=3)\n",
    "                if page_raw.status_code == 200:\n",
    "                    page_data = page_raw.json()\n",
    "                    with open(\"apidata/\"+str(page)+\".json\", 'w') as f:\n",
    "                        json.dump(page_data, f)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    def __read_json(self, filepath):\n",
    "        with open(filepath, 'r') as fp:\n",
    "                return json.load(fp)\n",
    "\n",
    "\n",
    "    def __check_directory(self, dirname):\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "            initials = self.__save_initial()\n",
    "            head = initials[\"head\"]\n",
    "            self.__save_rest(initials['totalPages'], head)\n",
    "\n",
    "\n",
    "    def get_usa(self):\n",
    "        dirname = 'apidata'\n",
    "        self.__check_directory(dirname)\n",
    "        complete_list = list()\n",
    "        # CHANGED '/' to os.sep so it would work for my Windows computer\n",
    "        files = [int(i.split(os.sep)[1].split('.')[0]) for i in glob(dirname+\"/*.json\")]\n",
    "        for num in sorted(files):\n",
    "            apifile = dirname+\"/\"+str(num)+\".json\"\n",
    "            file_json = self.__read_json(apifile)\n",
    "            file_data = file_json[\"data\"]\n",
    "            for item in file_data[\"items\"]:\n",
    "                out = dict()\n",
    "                detail = item[\"institution\"]\n",
    "                item_keys = [\"displayName\", \"rankingDisplayRank\", \"state\", \"city\", \"zip\"]\n",
    "                for key in item_keys:\n",
    "                    out[key] = detail[key]\n",
    "                out[\"description\"] = desc = item[\"blurb\"]\n",
    "                if desc.startswith(\"<p>\"):\n",
    "                    out[\"description\"] = desc[3:-4]\n",
    "                complete_list.append(out)\n",
    "        return complete_list\n",
    "\n",
    "\n",
    "    def get_names(self):\n",
    "        usa = self.get_usa()\n",
    "        names = [uni[\"displayName\"] for uni in usa]\n",
    "        return names\n",
    "\n",
    "\n",
    "    def get_top_names(self, num):\n",
    "        names = self.get_names()\n",
    "        return names[:num]\n",
    "\n",
    "\n",
    "    def select_by_state(self, state_list):\n",
    "        df = pd.DataFrame(self.get_usa())\n",
    "        sub = df[df[\"state\"].isin(state_list)]\n",
    "        return sub\n",
    "\n",
    "\n",
    "    def select_by_city(self, city_list):\n",
    "        df = pd.DataFrame(self.get_usa())\n",
    "        sub = df[df[\"city\"].isin(city_list)]\n",
    "        return sub\n",
    "\n",
    "\n",
    "    def save_json(self, inplist, filename):\n",
    "        if inplist != []:\n",
    "            with open(filename, 'w') as fh:\n",
    "                fh.write(json.dumps(inplist, indent=4))\n",
    "\n",
    "\n",
    "    def save_csv(self, inplist, filename):\n",
    "        if inplist != []:\n",
    "            df = pd.DataFrame(inplist)\n",
    "            df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate\n",
    "rank = Ranking()\n",
    "\n",
    "# get the ordered list of USA universities\n",
    "usa = rank.get_usa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank.save_csv(usa, \"usa_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll manually enter the dates each university moved online and required a vaccine using Excel. As there's no good centralized database, I'll look at the top 100 university websites myself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
