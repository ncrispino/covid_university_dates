{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predicting Booster\n",
    "Create a classification model that tells us whether or not universities mandate a booster *given* they already mandated a vaccine. Uses preprocessing from \"Covid Model Creation\" notebook. Note that this analysis better aligns with the iid assumption needed for machine learning--choosing dates to implement vaccine requirements is very much based on the action of other colleges. However, we can assume that choosing to implement a requirement is not *entirely* based on the actions of other institutions; once vaccination requirements seemed imminent, colleges evaluate their own situations and (likely) chose based on the science and social consequences for their students and surrounding environment, especially for a booster.\n",
    "\n",
    "Ideally, I'd like to train a model to classify the universities that required the vaccine and those that didn't. Then, I would want to try a multi-level classification with three options: one for no mandate, one for a regular mandate, and one for a booster mandate. However, the lack of schools in the data without a vaccine mandate makes this analysis more difficult. Since my dataset is small, the model will likely overfit on those small examples and provide bad generalization. Still, I may try this after I finish with my booster analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_booster = pd.read_pickle('target_booster.pkl')\n",
    "features_booster = pd.read_pickle('features_booster.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "categorical_preprocessor = OneHotEncoder(drop='first') # drop to avoid multicollinearity\n",
    "numerical_preprocessor = StandardScaler() # normalize data to make it easier for sklearn models to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer # splits the column, transforms each subset differently, then concatenates\n",
    "categorical_columns = ['ranking', 'Type', 'political_control_state', 'Region']\n",
    "numerical_columns = list(set(features_booster.columns).difference(categorical_columns))\n",
    "preprocessor = ColumnTransformer([('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "                                  ('standard_scaler', numerical_preprocessor, numerical_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "Accuracy is the fraction of cases identified correctly $= \\frac{tp + tn}{tp + tn + fp + fn}$\n",
    "\n",
    "Precision is the proportion of predicted positives that are true positives $= \\frac{tp}{tp + fp}$\n",
    "\n",
    "Recall is the proportion of true positives correctly identified $= \\frac{tp}{tp + fn}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See distribution of values to choose best metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.71223\n",
       "1    0.28777\n",
       "Name: booster, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_booster.value_counts()/target_booster.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have an imbalanced test set, meaning I won't use regular accuracy--I will use balanced accuracy instead.\n",
    "\n",
    "As I don't have a preference for precision or recall in this data, I'll use the F1 Score, which is the harmonic mean between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "def show_metrics_classification(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'balanced accuracy: {balanced_accuracy_score(y_test, y_pred)}')\n",
    "    print(f'F1 Score: {f1_score(y_test, y_pred)}') # equal weight to precision and recall\n",
    "    print(classification_report(y_test, y_pred, labels=[0, 1], target_names=['no booster', 'booster']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "Now train models. Using [Sebastian Raschka's paper as a reference](https://arxiv.org/abs/1811.12808), especially his [code on nested CV which I took heavily from](https://github.com/rasbt/model-eval-article-supplementary/blob/master/code/nested_cv_code.ipynb). I initially did some selection with the model (you can see my previous commits on GitHub), but am going to have a more comprehensive approach going forward. Here's a summary of some of the things discussed in the paper:\n",
    "\n",
    "Evaluate overall model performance:\n",
    "- Use Monte-Carlo Cross-Validation\n",
    "- Bootstrapping (LOOB) to ; use 50-200 samples\n",
    "- 3 way holdout -- used in deep learning when dataset is large\n",
    "- k-fold CV\n",
    "    - can repeat many times (unnecessary for LOOCV), e.g., run 5-fold cross validation 100 times (with different random seeds), getting 500 test fold estimates\n",
    "    - use LOOCV for small datasets--note it's approximately unbiased but with high variance\n",
    "    - generally, increasing k decreases bias but increases variance and computation time\n",
    "\n",
    "Hyperparameter tuning using CV:\n",
    "- find best params using k-fold CV, then fit model with those params to entire training set to evaluate test set, afterwards using all data to fit final model\n",
    "- feature selection could be done inside or outside the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Nested CV--outer loop estimates generalization error, inner loop selects model. For example, if we are doing 5-fold CV in the outer loop, we take the data from 4 folds, combine it into one dataset, then split that set into k folds and run CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_booster, target_booster, test_size=0.2, \n",
    "                                                    random_state=42, stratify=target_booster)\n",
    "# these will be passed to other methods to make sure the CV split is the same for every model\n",
    "inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up many models and grid search for each of them. After finding the best hyperparameters with the inner loop, train each model and evaluate on each of the k folds in the outer loop. Sklearn lets me do this nicely by passing in the grid search as a parameter for ```cross_val_score```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                  OneHotEncoder(drop='first'),\n",
       "                                                  ['ranking', 'Type',\n",
       "                                                   'political_control_state',\n",
       "                                                   'Region']),\n",
       "                                                 ('standard_scaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['total_population',\n",
       "                                                   '2020.student.size',\n",
       "                                                   'county_vote_diff',\n",
       "                                                   'announce_date',\n",
       "                                                   'avg_community_level',\n",
       "                                                   'median_income',\n",
       "                                                   'avg_hhsize'])])),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pipe_log = make_pipeline(preprocessor, LogisticRegression())\n",
    "pipe_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_log.score(X_test, y_test) # mean accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced accuracy: 0.6032608695652174\n",
      "F1 Score: 0.375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  no booster       0.71      0.96      0.81        23\n",
      "     booster       0.75      0.25      0.38        12\n",
      "\n",
      "    accuracy                           0.71        35\n",
      "   macro avg       0.73      0.60      0.59        35\n",
      "weighted avg       0.72      0.71      0.66        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_metrics_classification(pipe_log, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use random forests first with default setting, then with grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced accuracy: 0.5815217391304348\n",
      "F1 Score: 0.35294117647058826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  no booster       0.70      0.91      0.79        23\n",
      "     booster       0.60      0.25      0.35        12\n",
      "\n",
      "    accuracy                           0.69        35\n",
      "   macro avg       0.65      0.58      0.57        35\n",
      "weighted avg       0.67      0.69      0.64        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe_forest = make_pipeline(preprocessor, RandomForestClassifier())\n",
    "pipe_forest.fit(X_train, y_train)\n",
    "show_metrics_classification(pipe_forest, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do grid search, using oob error as the metric as in the \"Covid Model Creation\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "cv = PredefinedSplit([-1]*(X_train.shape[0]-1) + [0])\n",
    "for (train, test) in cv.split(X_train, y_train):\n",
    "    print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oob_scorer(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Get oob score where RandomForest is 1st element in Pipeline\n",
    "    \"\"\"\n",
    "    return estimator[1].oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also use [this post on tuning hyperparameters](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'randomforestclassifier__n_estimators': [25, 75, 100, 150, 200],\n",
    "     'randomforestclassifier__max_depth': [None, 10, 25, 50, 75, 100],\n",
    "     'randomforestclassifier__max_features': [None, 0.25, 0.5, 0.75], # Note I have 10 features (when they're not encoded)\n",
    "     'randomforestclassifier__min_samples_split': [2, 3, 5, 7], \n",
    "     'randomforestclassifier__min_samples_leaf': [1, 2, 3, 5] # 104 samples in training data -- 84 in validation\n",
    "    }\n",
    "]\n",
    "pipe_forest_grid = GridSearchCV(estimator=make_pipeline(preprocessor, RandomForestClassifier(oob_score=True)), param_grid=param_grid, scoring=oob_scorer, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1,  0])),\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                                         OneHotEncoder(drop='first'),\n",
       "                                                                         ['ranking',\n",
       "                                                                          'Type',\n",
       "                                                                          'political_control_state',\n",
       "                                                                          'Region']),\n",
       "                                                                        ('standard_scaler',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['total_population',\n",
       "                                                                          '2020.student.size',\n",
       "                                                                          'county_vote_diff',\n",
       "                                                                          'announ...\n",
       "                                        RandomForestClassifier(oob_score=True))]),\n",
       "             param_grid=[{'randomforestclassifier__max_depth': [None, 10, 25,\n",
       "                                                                50, 75, 100],\n",
       "                          'randomforestclassifier__max_features': [None, 0.25,\n",
       "                                                                   0.5, 0.75],\n",
       "                          'randomforestclassifier__min_samples_leaf': [1, 2, 3,\n",
       "                                                                       5],\n",
       "                          'randomforestclassifier__min_samples_split': [2, 3, 5,\n",
       "                                                                        7],\n",
       "                          'randomforestclassifier__n_estimators': [25, 75, 100,\n",
       "                                                                   150, 200]}],\n",
       "             scoring=<function oob_scorer at 0x000002782326B168>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_forest_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__max_depth': None,\n",
       " 'randomforestclassifier__max_features': 0.75,\n",
       " 'randomforestclassifier__min_samples_leaf': 5,\n",
       " 'randomforestclassifier__min_samples_split': 5,\n",
       " 'randomforestclassifier__n_estimators': 200}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(pipe_forest_grid.best_params_)\n",
    "pipe_forest_grid.score(X_test, y_test)\n",
    "pipe_forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(pipe_forest_grid.predict(X_test))\n",
    "print(pipe_forest.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced accuracy: 0.5815217391304348\n",
      "F1 Score: 0.35294117647058826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  no booster       0.70      0.91      0.79        23\n",
      "     booster       0.60      0.25      0.35        12\n",
      "\n",
      "    accuracy                           0.69        35\n",
      "   macro avg       0.65      0.58      0.57        35\n",
      "weighted avg       0.67      0.69      0.64        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_metrics_classification(pipe_forest_grid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I get the same outcome (or 1 prediction different) as the normal random forest, depending on when the tree splits (there are two similar outcomes). So, the hyperparameter tuning didn't change anything much at all.\n",
    "\n",
    "Now, I will use normal cross-validation, not the OOB error. However, I'll use random search so it's quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('columntransformer',\n",
       "                                              ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                                               OneHotEncoder(drop='first'),\n",
       "                                                                               ['ranking',\n",
       "                                                                                'Type',\n",
       "                                                                                'political_control_state',\n",
       "                                                                                'Region']),\n",
       "                                                                              ('standard_scaler',\n",
       "                                                                               StandardScaler(),\n",
       "                                                                               ['total_population',\n",
       "                                                                                '2020.student.size',\n",
       "                                                                                'county_vote_diff',\n",
       "                                                                                'announce_date',\n",
       "                                                                                'avg_community_level',\n",
       "                                                                                'median_inco...\n",
       "                   param_distributions=[{'randomforestclassifier__max_depth': [None,\n",
       "                                                                               10,\n",
       "                                                                               25,\n",
       "                                                                               50,\n",
       "                                                                               75,\n",
       "                                                                               100],\n",
       "                                         'randomforestclassifier__max_features': [None,\n",
       "                                                                                  0.25,\n",
       "                                                                                  0.5,\n",
       "                                                                                  0.75],\n",
       "                                         'randomforestclassifier__min_samples_leaf': [1,\n",
       "                                                                                      2,\n",
       "                                                                                      3,\n",
       "                                                                                      5],\n",
       "                                         'randomforestclassifier__min_samples_split': [2,\n",
       "                                                                                       3,\n",
       "                                                                                       5,\n",
       "                                                                                       7],\n",
       "                                         'randomforestclassifier__n_estimators': array([ 10,  22,  35,  47,  60,  73,  85,  98, 111, 123, 136, 148, 161,\n",
       "       174, 186, 199, 212, 224, 237, 250])}])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rand_param_grid = [\n",
    "    {'randomforestclassifier__n_estimators': np.linspace(10, 250, 20).astype(int),\n",
    "     'randomforestclassifier__max_depth': [None, 10, 25, 50, 75, 100],\n",
    "     'randomforestclassifier__max_features': [None, 0.25, 0.5, 0.75], # Note I have 10 features (when they're not encoded)\n",
    "     'randomforestclassifier__min_samples_split': [2, 3, 5, 7], \n",
    "     'randomforestclassifier__min_samples_leaf': [1, 2, 3, 5] # 104 samples in training data -- 84 in validation\n",
    "    }\n",
    "]\n",
    "forest_random_grid = RandomizedSearchCV(make_pipeline(preprocessor, RandomForestClassifier()), rand_param_grid, n_iter=20, cv=3)\n",
    "forest_random_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__n_estimators': 186,\n",
       " 'randomforestclassifier__min_samples_split': 3,\n",
       " 'randomforestclassifier__min_samples_leaf': 1,\n",
       " 'randomforestclassifier__max_features': 0.5,\n",
       " 'randomforestclassifier__max_depth': 25}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_random_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced accuracy: 0.6231884057971014\n",
      "F1 Score: 0.4444444444444444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  no booster       0.72      0.91      0.81        23\n",
      "     booster       0.67      0.33      0.44        12\n",
      "\n",
      "    accuracy                           0.71        35\n",
      "   macro avg       0.70      0.62      0.63        35\n",
      "weighted avg       0.70      0.71      0.68        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_metrics_classification(forest_random_grid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives an identical performing estimator to the random forest tuned using OOB error when using cv=5. I set cv=3, as I thought it might give different performance. As there's only 35 elements in the test set, using cv=5 I would only test on 7, where cv=3 means I would test on ~12. As there's already a class imbalance in the data, using more data is likely to get us a more representative sample and less likely to only include (or almost only include) colleges with booster mandates.\n",
    "\n",
    "Note that both ```GridSearchCV``` and ```RandomizedSearchCV``` both already use stratified CV to keep the percentage of samples in each class the same for each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using random forest methods gives a similar result to using logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ MORE on SVM before I do more\n",
    "Now, try SVM. I'll start with the defaults (rbf kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                  OneHotEncoder(drop='first'),\n",
       "                                                  ['ranking', 'Type',\n",
       "                                                   'political_control_state',\n",
       "                                                   'Region']),\n",
       "                                                 ('standard_scaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['total_population',\n",
       "                                                   '2020.student.size',\n",
       "                                                   'county_vote_diff',\n",
       "                                                   'announce_date',\n",
       "                                                   'avg_community_level',\n",
       "                                                   'median_income',\n",
       "                                                   'avg_hhsize'])])),\n",
       "                ('svc', SVC())])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "pipe_svm = make_pipeline(preprocessor, svm.SVC(kernel='rbf', gamma='scale'))\n",
    "pipe_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced accuracy: 0.5\n",
      "F1 Score: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  no booster       0.66      1.00      0.79        23\n",
      "     booster       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.66        35\n",
      "   macro avg       0.33      0.50      0.40        35\n",
      "weighted avg       0.43      0.66      0.52        35\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncris\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ncris\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ncris\\anaconda3\\envs\\deep_learn\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "show_metrics_classification(pipe_svm, X_test, y_test)\n",
    "print(pipe_svm.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('one-hot-encoder',\n",
       "                                                  OneHotEncoder(drop='first'),\n",
       "                                                  ['ranking', 'Type',\n",
       "                                                   'political_control_state',\n",
       "                                                   'Region']),\n",
       "                                                 ('standard_scaler',\n",
       "                                                  StandardScaler(),\n",
       "                                                  ['total_population',\n",
       "                                                   '2020.student.size',\n",
       "                                                   'county_vote_diff',\n",
       "                                                   'announce_date',\n",
       "                                                   'avg_community_level',\n",
       "                                                   'median_income',\n",
       "                                                   'avg_hhsize'])])),\n",
       "                ('svc', SVC(kernel='linear'))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_svm_lin = make_pipeline(preprocessor, svm.SVC(kernel='linear'))\n",
    "pipe_svm_lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced accuracy: 0.5597826086956521\n",
      "F1 Score: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "show_metrics_classification(pipe_svm_lin, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation and Training\n",
    "Evaluate the chosen model on the test set, then train the model using all the data to get a final predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Takeaway\n",
    "Create a mini web-page where people can input a state and zip-code and then our model can predict vaccine classification. \n",
    "\n",
    "Also, create a map, using slider bars to indicate university-specific variables not specified by state or county. \n",
    "\n",
    "Shade each region differently based on their classification. Have a drawing in my notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources (some in previous notebooks)\n",
    "- https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "deep_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
