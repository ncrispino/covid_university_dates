{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Predicting Booster\n",
    "Create a classification model that tells us whether or not universities mandate a booster *given* they already mandated a vaccine. Uses preprocessing from \"Covid Model Creation\" notebook. Note that this analysis better aligns with the iid assumption needed for machine learning--choosing dates to implement vaccine requirements is very much based on the action of other colleges. However, we can assume that choosing to implement a requirement is not *entirely* based on the actions of other institutions; once vaccination requirements seemed imminent, colleges evaluate their own situations and (likely) chose based on the science and social consequences for their students and surrounding environment, especially for a booster.\n",
    "\n",
    "Ideally, I'd like to train a model to classify the universities that required the vaccine and those that didn't. Then, I would want to try a multi-level classification with three options: one for no mandate, one for a regular mandate, and one for a booster mandate. However, the lack of schools in the data without a vaccine mandate makes this analysis more difficult. Since my dataset is small, the model will likely overfit on those small examples and provide bad generalization. Still, I may try this after I finish with my booster analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_booster = pd.read_pickle('target_booster.pkl')\n",
    "features_booster = pd.read_pickle('features_booster.pkl')\n",
    "num_features = features_booster.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "categorical_preprocessor = OneHotEncoder(drop='first') # drop to avoid multicollinearity\n",
    "numerical_preprocessor = StandardScaler() # normalize data to make it easier for sklearn models to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer # splits the column, transforms each subset differently, then concatenates\n",
    "categorical_columns = ['ranking', 'Type', 'political_control_state', 'Region']\n",
    "numerical_columns = list(set(features_booster.columns).difference(categorical_columns))\n",
    "preprocessor = ColumnTransformer([('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "                                  ('standard_scaler', numerical_preprocessor, numerical_columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See distribution of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.71223\n",
       "1    0.28777\n",
       "Name: booster, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_booster.value_counts()/target_booster.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is moderately imbalanced. So, using normal methods for classification won't always be optimal. According to [Daskalaki, Kopanas, & Avouris (2006)](https://www.tandfonline.com/doi/full/10.1080/08839510500313653), with imbalanced data it's important to choose a reasonable metric to evaluate performance, adjust the dataset so that the minority class is adequately represented or seen in predictions, and select the best algorithm, sometimes combining them if a single one's performance is not good enough.\n",
    "\n",
    "Specifically, we can change the cost of classifying minority/majority classes based on their prevalence and/or real-world implication, change the class distributions by undersampling the majority class or oversampling the minority class, use bagging or stacking to combine multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "Accuracy is the fraction of cases identified correctly $= \\frac{tp + tn}{tp + tn + fp + fn}$ \n",
    "\n",
    "Precision is the proportion of predicted positives that are true positives $= \\frac{tp}{tp + fp}$ use to minimize fp\n",
    "\n",
    "Recall is the proportion of true positives correctly identified $= \\frac{tp}{tp + fn}$ use to minimize fn\n",
    "\n",
    "F1 score is harmonic mean, but changes if confusion matrix flips.\n",
    "\n",
    "Matthews Correlation Coefficient (MCC) robust to changes in confusion matrix--treats fp and fn the same.\n",
    "\n",
    "ROC curve plots the true positive rate ($\\frac{tp}{tp + fn}$) against the false positive rate ($\\frac{fp}{fp + tn}$) according to the probability thresholds for classification. Take the area under the curve (AOC) to compare.\n",
    "\n",
    "Precision/Recall curve plots precision against recall. Better than ROC curve if class imbalance exists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have an imbalanced dataset, meaning I won't use regular accuracy.\n",
    "\n",
    "Note that in previous attempts, I have just used accuracy as the metric to measure classification performance. This has resulted in very similar results to the dummy classifier; in some cases, the dummy classifier is better according to this metric. However, due to the class imbalance, I will use another metric. I want the positive examples (having a booster) to be classified as well--not just the negative examples as with the dummy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy | Outer accuracy: 0.7118577075098814 +/- 0.019379684857124557\n",
      "Dummy | Outer AUC: 0.7118577075098814 +/- 0.019379684857124557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dclf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_scores = cross_val_score(dclf, X=X_train, y=y_train, cv=inner_cv)\n",
    "print(f'Dummy | Outer accuracy: {dummy_scores.mean()} +/- {dummy_scores.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Boughorbel, Jarray, & El-Anbari](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177678), MCC is shown to be \"robust to data imbalance\" along with AUC, but has a closed form which is easier to compute. MCC is related to the Pearson correlation coefficient, defined as $\\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FN)(TP + FP)(TN + FP)(TN+FN)}} \\in [-1, 1]$ where 1 is perfect, -1 is inverse and 0 is average. Also, [Chicco, TÃ¶tsch, & Jurman (2021)](https://biodatamining.biomedcentral.com/articles/10.1186/s13040-021-00244-z) think MCC is generally the best single value evaluator of performance when *equal importance is given to both classes*. For example, if the goal is to minimize false positives, a different metric will be better. A high MCC \n",
    "> \"means always high values for all the four basic rates of the confusion matrix: true\n",
    "positive rate (TPR), true negative rate (TNR), positive predictive value (PPV), and negative predictive value (NPV)\" \n",
    "\n",
    "Thus the other classification metrics will also be high. Note that bookmaker informedness (BM) should be used instead if classifiers are compared across datasets or if we are comparing the classifier to random guessing; however, here we are comparing with the same dataset and care about overall performance so MCC will be better and will be used in my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy | Outer MCC: 0.0 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import make_scorer\n",
    "mcc_score = make_scorer(matthews_corrcoef, greater_is_better=True)\n",
    "\n",
    "# test: should be 0 as no negatives predicted\n",
    "dummy_scores_mcc = cross_val_score(dclf, X=X_train, y=y_train, cv=inner_cv, scoring=mcc_score)\n",
    "print(f'Dummy | Outer MCC: {dummy_scores_mcc.mean()} +/- {dummy_scores_mcc.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "Now train models. Using [Sebastian Raschka's paper as a reference](https://arxiv.org/abs/1811.12808), especially his [code on nested CV which I took heavily from](https://github.com/rasbt/model-eval-article-supplementary/blob/master/code/nested_cv_code.ipynb). I initially did some selection with the model (you can see my previous commits on GitHub), but am going to have a more comprehensive approach going forward. Here's a summary of some of the things discussed in the paper:\n",
    "\n",
    "Evaluate overall model performance:\n",
    "- Use Monte-Carlo Cross-Validation\n",
    "- Bootstrapping (LOOB) to ; use 50-200 samples\n",
    "- 3 way holdout -- used in deep learning when dataset is large\n",
    "- k-fold CV\n",
    "    - can repeat many times (unnecessary for LOOCV), e.g., run 5-fold cross validation 100 times (with different random seeds), getting 500 test fold estimates\n",
    "    - use LOOCV for small datasets--note it's approximately unbiased but with high variance\n",
    "    - generally, increasing k decreases bias but increases variance and computation time\n",
    "\n",
    "Hyperparameter tuning using CV:\n",
    "- find best params using k-fold CV, then fit model with those params to entire training set to evaluate test set, afterwards using all data to fit final model\n",
    "- feature selection could be done inside or outside the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Nested CV--outer loop estimates generalization error, inner loop selects model. For example, if we are doing 5-fold CV in the outer loop, we take the data from 4 folds, combine it into one dataset, then split that set into k folds and run CV. This will better account for the variance of the test set--same motivation as regular cross validation accounting for the variance of the validation set. Also note that in the previous example we select 5 best models and see the generalization error on each of them. We can choose one of those models or ensemble those models. See [Sergey Feldman's lecture](https://www.youtube.com/watch?v=DuDtXtKNpZs) for more.\n",
    "\n",
    "**Figuring** out what to do after nested cross validation has been difficult. This is a summary of the previous resources and the stack overflow articles listed here ([1](https://stats.stackexchange.com/questions/232897/how-to-build-the-final-model-and-tune-probability-threshold-after-nested-cross-v/233027#233027), [2](https://stats.stackexchange.com/questions/65128/nested-cross-validation-for-model-selection/65156#65156), [3](https://stats.stackexchange.com/questions/341229/an-intuitive-understanding-of-each-fold-of-a-nested-cross-validation-for-paramet?rq=1), [4](https://stats.stackexchange.com/questions/244907/how-to-get-hyper-parameters-in-nested-cross-validation/245169#245169)):\n",
    "\n",
    "My takeaways are that the inner loop is for model selection, while the outer loop is for generalization error. Therefore it would be wrong to choose a model based on the results of the outer loop. Instead, the estimates of the all the errors in outer loop for a specific model are averaged to provide the approximate generalization error for that model fitting method when all the data is used. After nested CV is done, apply your inner CV methods with all the data to select the optimal hyperparameters, using the results from nested CV as estimates for this procedure.\n",
    "From 3:\n",
    ">Thus: run the auto-tuning of hyperparameters on the whole data set just as you do during cross validation. Same hyperparameter combinations to consider, same strategy for selecting the optimum. In short: same training algorithm, just slightly different data (1/k additional cases).\n",
    "\n",
    "\n",
    "\n",
    "Note that if the inner and outer estimates of the model are very different, this could signify overfitting. Also note if the inner estimates vary widely in their hyperparameters, this means the model is likely not stable. Can use iterated/repeated CV to inspect further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_booster, target_booster, test_size=0.2, \n",
    "                                                    random_state=42, stratify=target_booster)\n",
    "# use 5-fold for inner and outer so there's enough data in validation set but not too much\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up many models and grid search for each of them. After finding the best hyperparameters with the inner loop, train each model and evaluate on each of the k folds in the outer loop. Sklearn lets me do this nicely by passing in the grid search as a parameter for ```cross_val_score```.\n",
    "\n",
    "Create all the models I want to use and a parameter grid for each of them. I'll use logistic regression, random forest, and SVM. Nested CV should give 3\\*k total estimates of model performance--k for each algorithm, where k is the number of folds in the outer CV. I'll average the estimates by model and report the standard deviation for each of the 3 models. Then, I'll choose the best model according to this and apply CV (the same as the inner CV used previously but with more data) with the same grid search parameters on the training set. Note that the generalization errors are no longer unbiased once I've chosen the minimum. Finally, I'll train those parameters on the entire training set, and use the test set to get an unbiased estimtae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log: [0.03742406 0.18401748 0.35634832 0.55901699 0.3125    ]\n",
      "for: [0.10956262 0.43704152 0.35147975 0.01992048 0.35147975]\n",
      "svm: [0.03742406 0.0270666  0.16137431 0.50952467 0.24056261]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model_names = ['log', 'for', 'svm']\n",
    "\n",
    "pipe_log = Pipeline([('pre', preprocessor), (model_names[0], LogisticRegression(random_state=42, solver='liblinear'))])\n",
    "pipe_for = Pipeline([('pre', preprocessor), (model_names[1], RandomForestClassifier(random_state=42, oob_score=True))])\n",
    "pipe_svm = Pipeline([('pre', preprocessor), (model_names[2], SVC(random_state=42))])\n",
    "param_grid_log = [\n",
    "    {'log__C': [0.1**(x) for x in range(-4, 4)]} # l2 default\n",
    "] \n",
    "param_grid_for = [\n",
    "    {'for__max_features': [0.5, 0.75, 'sqrt', None], \n",
    "     'for__max_samples': [0.25, 0.5, 0.75, None],\n",
    "     'for__min_samples_leaf': [1, 2, 5],\n",
    "#      'for__max_leaf_nodes': [None, 2, 5, 10], \n",
    "     'for__max_depth': [10, 25, 50, None],\n",
    "     # 'for__min_samples_split': [2, 3, 5, 7, 10],\n",
    "    }\n",
    "]\n",
    "param_grid_svm = [\n",
    "    {'svm__kernel': ['linear', 'poly', 'rbf'],\n",
    "     'svm__gamma': [2**x for x in range(-12, 2, 2)],\n",
    "     'svm__C': [2**x for x in range(-4, 10, 2)]\n",
    "    }\n",
    "]\n",
    "\n",
    "def oob_scorer(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Get oob score where RandomForest is 1st element in Pipeline\n",
    "    \"\"\"\n",
    "    return estimator[1].oob_score_\n",
    "\n",
    "validation_scores = np.zeros((3, 5))\n",
    "for i, pipe, param_grid in zip([0, 1, 2], [pipe_log, pipe_for, pipe_svm], [param_grid_log, param_grid_for, param_grid_svm]):\n",
    "    name = list(pipe.named_steps.keys())[1]\n",
    "#     if name == 'for': # use oob error instead of CV estimates for inner CV--but this would require changing CV split which I'll ignore for now\n",
    "#         gcv = GridSearchCV(pipe, param_grid, cv=inner_cv, scoring=oob_scorer)\n",
    "#     else:\n",
    "    gcv = GridSearchCV(pipe, param_grid, cv=inner_cv, scoring=mcc_score)\n",
    "    nested_score = cross_val_score(gcv, X=X_train, y=y_train, cv=outer_cv, scoring=mcc_score)\n",
    "    validation_scores[i, :] = nested_score\n",
    "    print(f'{name}: {nested_score}') # should output k_outer folds--the number of folds used in outer cv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log | Outer MCC: 0.28986137176887944 +/- 0.17453096965981016\n",
      "for | Outer MCC: 0.2538968223656043 +/- 0.1601025458955238\n",
      "svm | Outer MCC: 0.19519044822913645 +/- 0.17616130517775808\n"
     ]
    }
   ],
   "source": [
    "validation_scores_mean = validation_scores.mean(axis=1)\n",
    "validation_scores_std = validation_scores.std(axis=1)\n",
    "for i in range(3):\n",
    "    print(f'{model_names[i]} | Outer MCC: {validation_scores_mean[i]} +/- {validation_scores_std[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I have an approximately unbiased generalization error for all three model selecting processes. Logistic regression and random forests have a similar estimated MCC and a similar standard deviation. SVM performs worse and has a similar standard deviation. Now run my model selection procedure on the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gcv = GridSearchCV(pipe_log, param_grid_log, cv=inner_cv, scoring=mcc_score)\n",
    "final_gcv.fit(X_train, y_train)\n",
    "final_gcv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resampling (under/oversampling) or add more data if classifier not good enough**\n",
    "[class weights?](https://stackoverflow.com/questions/30972029/how-does-the-class-weight-parameter-in-scikit-learn-work)\n",
    "\n",
    "also, use bayesian optimization instead of grid search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation and Training\n",
    "Evaluate the chosen model on the test set, then train the model using all the data to get a final predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Takeaway\n",
    "Create a mini web-page where people can input a state and zip-code and then our model can predict vaccine classification. \n",
    "\n",
    "Also, create a map, using slider bars to indicate university-specific variables not specified by state or county. \n",
    "\n",
    "Shade each region differently based on their classification. Have a drawing in my notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources (some in previous notebooks)\n",
    "- Metrics for classification\n",
    "    - https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n",
    "    - https://www.youtube.com/watch?v=wpQiEHYkBys\n",
    "    - https://www.youtube.com/watch?v=X9MZtvvQDR4 for imbalanced datasets\n",
    "    - https://stats.stackexchange.com/questions/391064/evaluating-classifiers-k-fold-cv-or-roc\n",
    "    - https://www.kaggle.com/general/7517\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef\n",
    "- random forest parameter tuning\n",
    "    - https://stats.stackexchange.com/questions/344220/how-to-tune-hyperparameters-in-a-random-forest\n",
    "    - https://arxiv.org/pdf/1804.03515.pdf\n",
    "- svm parameter tuning\n",
    "    - https://stats.stackexchange.com/questions/43943/which-search-range-for-determining-svm-optimal-c-and-gamma-parameters\n",
    "    - https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\n",
    "    - https://stats.stackexchange.com/questions/249881/svm-hyperparameters-tuning use bayesian optimization in future?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn",
   "language": "python",
   "name": "deep_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
